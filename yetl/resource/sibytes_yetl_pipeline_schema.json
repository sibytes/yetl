{
    "$id": "https://yetl.io/schemas/pipeline",

    "type":"object",
    "description": "Root of the yetl tables config",
    "properties": {
        "version": {
            "type": "string",
            "description": "version of yetl that the configuration is compatible with",
            "pattern": "^(\\d+\\.)?(\\d+\\.)?(\\*|\\d+)$"
        },
        "audit_control": {
            "type": "object",
            "description": "definition of the audit_control database and tables",
            "properties": {
                "delta_lake": { "$ref": "#/$defs/delta_lake" }
            }
        },
        "source": {
            "type": "object",
            "description": "definition of the landing stage and files",
            "properties": {
                "delta_lake": { "$ref": "#/$defs/delta_lake" },
                "read": { "$ref": "#/$defs/read" }
            },
            "oneOf": [
                {
                    "required":[
                        "read"
                    ]
                },
                {
                    "required":[
                        "delta_lake"
                    ]
                }
            ]
        },
        "landing": {
            "type": "object",
            "description": "definition of the landing stage and files",
            "properties": {
                "read": { "$ref": "#/$defs/read" }
            },
            "required":[
                "read"
            ]
        },

        "raw": {
            "type": "object",
            "description": "definition of the raw database and tables",
            "properties": {
                "delta_lake": { "$ref": "#/$defs/delta_lake" }
            }
        },
        "base": {
            "type": "object",
            "description": "definition of the base database and tables",
            "properties": {
                "delta_lake": { "$ref": "#/$defs/delta_lake" }
            }
        }
    },
    "required": [
        "version",
        "landing"
    ],

    "$defs": {
        "read": {
            "type": "object",
            "description": "read table type is used for spark read table properties, typically used for reading files in object storage",
            "properties": {
                "trigger": {
                    "type": ["string", "null"],
                    "description": "filemask patter to use as a trgger"
                },
                "trigger_type": {
                    "type": ["string", "null"],
                    "description": "type of trgger"
                },
                "container": {
                    "type": ["string", "null"],
                    "description": "type of trgger"
                },
                "location": {
                    "type": ["string", "null"],
                    "description": "file directory location"
                },
                "filename": {
                    "type": ["string", "null"],
                    "description": "filename mask"
                },
                "filename_date_format": {
                    "type": ["string", "null"],
                    "description": "define a date format jinja variable for filename dates"
                },
                "path_date_format": {
                    "type": ["string", "null"],
                    "description": "define a date format jinja variable for file paths"
                },
                "slice_date": {
                    "type": "string",
                    "enum": ["filename_date_format", "path_date_format"],
                    "description": "either the filename_date_format or the path_date_format used to shred the time period from the filename or path respectively"
                },
                "format": {
                    "type": "string",
                    "enum": ["cloudFiles", "csv", "json", "parquet"],
                    "description": "format of the landing file"
                },
                "spark_schema": {
                    "type": "string",
                    "description": "relative path to where the spark definition is held"
                },
                "options": { "$ref": "#/$defs/options" }
            }
        },

        "delta_lake": {
            "type": "object",
            "description": "defines a stage as a delta lake table stage",
            "properties": {
                "managed": {
                    "type": "boolean",
                    "description": "whether it's a managed table or unmanaged that requires a location"
                },
                "delta_properties": { "$ref": "#/$defs/delta_properties" },
                "exception_thresholds": { "$ref": "#/$defs/thresholds" },
                "warning_thresholds": { "$ref": "#/$defs/thresholds" },
                "container": {
                    "type": ["string", "null"],
                    "description": "type of trgger"
                },
                "location": {
                    "type": ["string", "null"],
                    "description": "file location of managed tables for the stage"
                },
                "path": {
                    "type": ["string", "null"],
                    "description": "path of the table appended to the location of the stage"
                },
                "options": { "$ref": "#/$defs/options" },
                "database": {
                    "type": "string",
                    "description": "name of the database, {{ database }} variable will inherit the database name from table configuration"
                },
                "table": {
                    "type": "string",
                    "description": "name of the database table, {{ table }} variable will inherit the database name from table configuration"
                }
            }
        },
        "options": {
            "type": ["object","null"],
            "description": "holds key value pairs of custom properties",
            "minProperties": 1,
            "patternProperties":{
                "^\\S+$": {
                    "type": ["string","number","boolean"],
                    "description": "value kay pairs of the spark DSL read options"
                }
            }
        },
        "thresholds": {
            "type": "object",
            "description": "table etl thresholds",
            "properties": {
                "invalid_ratio": {
                    "type": "number", 
                    "description": "decimal between 0 and 1 specifying the ratio of invalid rows to valid rows threshold",
                    "exclusiveMinimum": 0,
                    "maximum": 1
                },
                "invalid_rows": {"type": "integer", "description": "integer specifying invalid rows threshold"},
                "max_rows": {"type": "integer", "description": "integer specifying max rows threshold"},
                "min_rows": {"type": "integer", "description": "integer specifying min rows threshold"}
            }
        },
        "delta_properties": {
            "type": "object",
            "description": "holds key value pairs of delta properties",
            "minProperties": 1,
            "properties":{
                "delta.appendOnly": {
                    "type": "boolean",
                    "description": "true for this Delta table to be append-only. If append-only, existing records cannot be deleted, and existing values cannot be updated."
                },
                "delta.autoOptimize.autoCompact": {
                    "type": ["string","boolean"],
                    "description": "auto for Delta Lake to automatically optimize the layout of the files for this Delta table."
                },    
                "delta.autoOptimize.optimizeWrite": {
                    "type": ["string","boolean"],
                    "description": "true for Delta Lake to automatically optimize the layout of the files for this Delta table during writes."
                },    
                "delta.checkpoint.writeStatsAsJson": {
                    "type": ["string","boolean"],
                    "description": "true for Delta Lake to write file statistics in checkpoints in JSON format for the stats column."
                },
                "delta.checkpoint.writeStatsAsStruct": {
                    "type": ["string","boolean"],
                    "description": "true for Delta Lake to write file statistics to checkpoints in struct format for the stats_parsed column and to write partition values as a struct for partitionValues_parsed."
                },
                "delta.columnMapping.mode": {
                    "type": "string",
                    "description": "Whether column mapping is enabled for Delta table columns and the corresponding Parquet columns that use different names."
                },
                "delta.compatibility.symlinkFormatManifest.enabled": {
                    "type": ["string","boolean"],
                    "description": "true for Delta Lake to configure the Delta table so that all write operations on the table automatically update the manifests."
                },
                "delta.dataSkippingNumIndexedCols": {
                    "type": "integer",
                    "description": "The number of columns for Delta Lake to collect statistics about for data skipping. A value of -1 means to collect statistics for all columns. Updating this property does not automatically collect statistics again; instead, it redefines the statistics schema of the Delta table. Specifically, it changes the behavior of future statistics collection (such as during appends and optimizations) as well as data skipping (such as ignoring column statistics beyond this number, even when such statistics exist)."
                },
                "delta.deletedFileRetentionDuration": {
                    "type": "integer",
                    "description": "The shortest duration for Delta Lake to keep logically deleted data files before deleting them physically. This is to prevent failures in stale readers after compactions or partition overwrites."
                },
                "delta.enableChangeDataFeed": {
                    "type": ["string","boolean"],
                    "description": "true to enable change data feed."
                },
                "delta.isolationLevel":  {
                    "type": "string",
                    "description": "The degree to which a transaction must be isolated from modifications made by concurrent transactions."
                },
                "delta.logRetentionDuration": {
                    "type": "string",
                    "description": "How long the history for a Delta table is kept. VACUUM operations override this retention threshold."
                },
                "delta.minReaderVersion": {
                    "type": "integer",
                    "description": "The minimum required protocol reader version for a reader that allows to read from this Delta table."
                },
                "delta.minWriterVersion": {
                    "type": "integer",
                    "description": "The minimum required protocol writer version for a writer that allows to write to this Delta table."
                },
                "delta.randomizeFilePrefixes": {
                    "type": ["string","boolean"],
                    "description": "true for Delta Lake to generate a random prefix for a file path instead of partition information."
                },
                "delta.randomPrefixLength": {
                    "type": "integer",
                    "description": "When delta.randomizeFilePrefixes is set to true, the number of characters that Delta Lake generates for random prefixes."
                },
                "delta.setTransactionRetentionDuration": {
                    "type": "string",
                    "description": "The shortest duration within which new snapshots will retain transaction identifiers (for example, SetTransactions). When a new snapshot sees a transaction identifier older than or equal to the duration specified by this property, the snapshot considers it expired and ignores it. The SetTransaction identifier is used when making the writes idempotent. "
                },
                "delta.targetFileSize": {
                    "type": "string",
                    "description": "The target file size in bytes or higher units for file tuning. For example, 104857600 (bytes) or 100mb."
                },
                "delta.tuneFileSizesForRewrites": {
                    "type": ["string","boolean"],
                    "description": "true to always use lower file sizes for all data layout optimization operations on the Delta table."
                }
            }
        }
    }
}