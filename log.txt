INFO : 2022-07-26 07:19:21,674 : __main__ : main.py.<module>: line(37) : Executing pipeline test_customer_landing_to_rawdb
INFO : 2022-07-26 07:19:21,675 : customer_landing_to_rawdb_csv : _config_provider.py.load_config: line(40) : Loading Dataflow configuration from file ./config/pipeline/local/config.yaml
:: loading settings :: url = jar:file:/Users/shaunryan/opt/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
INFO : 2022-07-26 07:19:25,884 : customer_landing_to_rawdb_csv : _context.py.__init__: line(45) : Setting application context spark logger at level ERROR
INFO : 2022-07-26 07:19:25,888 : customer_landing_to_rawdb_csv : _factory.py.get_file_system_type: line(38) : Setting filestystem using protocol file:
INFO : 2022-07-26 07:19:25,888 : customer_landing_to_rawdb_csv : _context.py.__init__: line(59) : Setting application context dataflow customer_landing_to_rawdb_csv
INFO : 2022-07-26 07:19:25,888 : customer_landing_to_rawdb_csv : _config_provider.py.load_pipeline_config: line(28) : Loading Dataflow configuration from file ./config/pipeline/local/customer_landing_to_rawdb_csv.yaml
INFO : 2022-07-26 07:19:25,906 : customer_landing_to_rawdb_csv : _factory.py.get_schema_repo_type: line(39) : Setting up schema repo on spark_schema_file 
WARNING : 2022-07-26 07:19:25,910 : customer_landing_to_rawdb_csv : _reader.py._is_corrupt_column_set: line(135) : mode=permissive and corrupt record is not set in the schema, schema on read corrupt records will be silently dropped.
INFO : 2022-07-26 07:19:25,910 : customer_landing_to_rawdb_csv : _reader.py.__init__: line(50) : auto_io = True automatically creating or altering exception delta table landing.customer _correlation_id=2cdfb323-eb8c-490c-9cd9-56987b52ed66
INFO : 2022-07-26 07:19:25,910 : customer_landing_to_rawdb_csv : _delta_lake.py.create_database: line(27) : Creating database if not exists `landing`
INFO : 2022-07-26 07:19:31,750 : customer_landing_to_rawdb_csv : _reader.py.create_or_alter_table: line(62) : Exception table already exists landing.exceptions at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/exceptions/landing/exceptions _correlation_id=2cdfb323-eb8c-490c-9cd9-56987b52ed66
INFO : 2022-07-26 07:19:31,750 : customer_landing_to_rawdb_csv : _reader.py.read: line(193) : Reading data for landing.customer from file:/Users/shaunryan/AzureDevOps/yetl/data/landing/20220101/customer_20220101.csv with options {'mode': 'PERMISSIVE', 'inferSchema': False, 'header': True} _correlation_id=2cdfb323-eb8c-490c-9cd9-56987b52ed66
INFO : 2022-07-26 07:19:32,132 : customer_landing_to_rawdb_csv : _factory.py.get_schema_repo_type: line(39) : Setting up schema repo on spark_schema_file 
INFO : 2022-07-26 07:19:32,135 : customer_landing_to_rawdb_csv : _reader.py.__init__: line(50) : auto_io = True automatically creating or altering exception delta table landing.customer_preferences _correlation_id=2cdfb323-eb8c-490c-9cd9-56987b52ed66
INFO : 2022-07-26 07:19:32,135 : customer_landing_to_rawdb_csv : _delta_lake.py.create_database: line(27) : Creating database if not exists `landing`
INFO : 2022-07-26 07:19:32,253 : customer_landing_to_rawdb_csv : _reader.py.create_or_alter_table: line(62) : Exception table already exists landing.exceptions at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/exceptions/landing/exceptions _correlation_id=2cdfb323-eb8c-490c-9cd9-56987b52ed66
INFO : 2022-07-26 07:19:32,253 : customer_landing_to_rawdb_csv : _reader.py.read: line(193) : Reading data for landing.customer_preferences from file:/Users/shaunryan/AzureDevOps/yetl/data/landing/20220101/customer_preferences_20220101.csv with options {'mode': 'PERMISSIVE', 'inferSchema': False, 'header': True} _correlation_id=2cdfb323-eb8c-490c-9cd9-56987b52ed66
INFO : 2022-07-26 07:19:32,303 : customer_landing_to_rawdb_csv : _reader.py.validate: line(179) : Validating dataframe read using PERMISSIVE corrupt column at _corrupt_record _correlation_id=2cdfb323-eb8c-490c-9cd9-56987b52ed66
INFO : 2022-07-26 07:19:33,271 : customer_landing_to_rawdb_csv : _reader.py.validate: line(190) : {
    "validation": {
        "schema_on_read": {
            "landing.customer_preferences": {
                "total_count": 10,
                "valid_count": 10,
                "exception_count": 0
            }
        }
    }
}
INFO : 2022-07-26 07:19:33,271 : customer_landing_to_rawdb_csv : _factory.py.get_schema_repo_type: line(39) : Setting up schema repo on deltalake_sql_file 
INFO : 2022-07-26 07:19:33,272 : customer_landing_to_rawdb_csv : _writer.py.__init__: line(39) : auto_io = True automatically creating or altering delta table raw.customer
INFO : 2022-07-26 07:19:33,272 : customer_landing_to_rawdb_csv : _delta_lake.py.create_database: line(27) : Creating database if not exists `raw`
INFO : 2022-07-26 07:19:33,360 : customer_landing_to_rawdb_csv : _writer.py.create_or_alter_table: line(87) : Table already exists raw.customer at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/raw/customer
INFO : 2022-07-26 07:19:33,360 : customer_landing_to_rawdb_csv : _delta_lake.py.get_table_properties: line(69) : getting existing table properties for table raw.customer
INFO : 2022-07-26 07:19:39,547 : customer_landing_to_rawdb_csv : _delta_lake.py.get_table_properties: line(91) : {
    "raw.customer": {
        "constraints": {
            "amount_lower_bound": "amount > - 1000",
            "amount_upper_bound": "amount < 10000"
        },
        "properties": {
            "Type": "EXTERNAL",
            "delta.appendOnly": "False",
            "delta.checkpoint.writeStatsAsJson": "True",
            "delta.compatibility.symlinkFormatManifest.enabled": "False",
            "delta.dataSkippingNumIndexedCols": "-1",
            "delta.deletedFileRetentionDuration": "interval 1 week",
            "delta.enableChangeDataFeed": "True",
            "delta.logRetentionDuration": "interval 30 days",
            "delta.minReaderVersion": "1",
            "delta.minWriterVersion": "4",
            "delta.randomPrefixLength": "2",
            "delta.randomizeFilePrefixes": "False",
            "external": "true"
        }
    }
}
INFO : 2022-07-26 07:19:47,072 : customer_landing_to_rawdb_csv : main.py.customer_landing_to_rawdb_csv: line(15) : Executing Dataflow customer_landing_to_rawdb_csv with timeslice=2022-01-01 00:00:00, retries=2
INFO : 2022-07-26 07:19:47,089 : customer_landing_to_rawdb_csv : _writer.py.write: line(183) : Writing data to raw.customer at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/raw/customer
ERROR : 2022-07-26 07:19:47,859 : customer_landing_to_rawdb_csv : _framework.py.wrap_function: line(39) : Dataflow application customer_landing_to_rawdb_csv failed due to An error occurred while calling o119.save.
: org.apache.spark.sql.delta.schema.DeltaInvariantViolationException: CHECK constraint amount_lower_bound (amount > -1000) violated by row with values:
 - amount : null
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException$.apply(InvariantViolationException.scala:60)
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException$.apply(InvariantViolationException.scala:70)
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException.apply(InvariantViolationException.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.CheckDeltaInvariant_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.$anonfun$doExecute$3(DeltaInvariantCheckerExec.scala:87)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
.
INFO : 2022-07-26 07:19:47,859 : customer_landing_to_rawdb_csv : _framework.py.wrap_function: line(42) : Retrying customer_landing_to_rawdb_csv in after 1 seconds; 1 retries are remaning.
INFO : 2022-07-26 07:19:48,861 : customer_landing_to_rawdb_csv : main.py.customer_landing_to_rawdb_csv: line(15) : Executing Dataflow customer_landing_to_rawdb_csv with timeslice=2022-01-01 00:00:00, retries=1
INFO : 2022-07-26 07:19:48,870 : customer_landing_to_rawdb_csv : _writer.py.write: line(183) : Writing data to raw.customer at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/raw/customer
ERROR : 2022-07-26 07:19:49,173 : customer_landing_to_rawdb_csv : _framework.py.wrap_function: line(39) : Dataflow application customer_landing_to_rawdb_csv failed due to An error occurred while calling o127.save.
: org.apache.spark.sql.delta.schema.DeltaInvariantViolationException: CHECK constraint amount_lower_bound (amount > -1000) violated by row with values:
 - amount : null
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException$.apply(InvariantViolationException.scala:60)
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException$.apply(InvariantViolationException.scala:70)
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException.apply(InvariantViolationException.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.CheckDeltaInvariant_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.$anonfun$doExecute$3(DeltaInvariantCheckerExec.scala:87)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
.
INFO : 2022-07-26 07:19:49,174 : customer_landing_to_rawdb_csv : _framework.py.wrap_function: line(42) : Retrying customer_landing_to_rawdb_csv in after 1 seconds; 0 retries are remaning.
INFO : 2022-07-26 07:19:50,179 : customer_landing_to_rawdb_csv : main.py.customer_landing_to_rawdb_csv: line(15) : Executing Dataflow customer_landing_to_rawdb_csv with timeslice=2022-01-01 00:00:00, retries=0
INFO : 2022-07-26 07:19:50,188 : customer_landing_to_rawdb_csv : _writer.py.write: line(183) : Writing data to raw.customer at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/raw/customer
ERROR : 2022-07-26 07:19:50,450 : customer_landing_to_rawdb_csv : _framework.py.wrap_function: line(39) : Dataflow application customer_landing_to_rawdb_csv failed due to An error occurred while calling o135.save.
: org.apache.spark.sql.delta.schema.DeltaInvariantViolationException: CHECK constraint amount_lower_bound (amount > -1000) violated by row with values:
 - amount : null
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException$.apply(InvariantViolationException.scala:60)
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException$.apply(InvariantViolationException.scala:70)
	at org.apache.spark.sql.delta.schema.DeltaInvariantViolationException.apply(InvariantViolationException.scala)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.CheckDeltaInvariant_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.$anonfun$doExecute$3(DeltaInvariantCheckerExec.scala:87)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
.
