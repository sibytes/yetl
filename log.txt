INFO : 2022-09-04 20:47:49,008 : customer_landing_to_rawdb_csv : _config_provider.py.load_config: line(41) : Loading Dataflow configuration from file /Users/shaunryan/AzureDevOps/yetl/config/pipeline/local/config.yaml
INFO : 2022-09-04 20:47:49,011 : customer_landing_to_rawdb_csv : _context.py._get_spark_context: line(123) : Setting spark context
22/09/04 20:47:51 WARN Utils: Your hostname, Shauns-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.16 instead (on interface en0)
22/09/04 20:47:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/Users/shaunryan/opt/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
22/09/04 20:47:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
INFO : 2022-09-04 20:47:53,869 : customer_landing_to_rawdb_csv : _context.py.__init__: line(52) : Setting application context spark logger at level ERROR
INFO : 2022-09-04 20:47:53,881 : customer_landing_to_rawdb_csv : _factory.py.get_file_system_type: line(50) : Setting filestystem using protocol file:
INFO : 2022-09-04 20:47:53,881 : customer_landing_to_rawdb_csv : _factory.py.get_metadata_repo_type: line(41) : Setting up metadata repo on metadata_file 
INFO : 2022-09-04 20:47:53,881 : customer_landing_to_rawdb_csv : _context.py.__init__: line(73) : Setting application context dataflow customer_landing_to_rawdb_csv
INFO : 2022-09-04 20:47:53,882 : customer_landing_to_rawdb_csv : _config_provider.py.load_pipeline_config: line(29) : Loading Dataflow configuration from file /Users/shaunryan/AzureDevOps/yetl/config/pipeline/local/customer_landing_to_rawdb_csv.yaml
INFO : 2022-09-04 20:47:53,902 : customer_landing_to_rawdb_csv : _factory.py.get_schema_repo_type: line(39) : Setting up schema repo on spark_schema_file 
INFO : 2022-09-04 20:47:53,902 : customer_landing_to_rawdb_csv : _spark_file_schema_repo.py.load_schema: line(58) : Loading schema for dataset landing.customer from /Users/shaunryan/AzureDevOps/yetl/config/schema/spark/landing/customer.yaml using <class 'yetl_flow.file_system._file_system.FileSystem'>
INFO : 2022-09-04 20:47:53,907 : customer_landing_to_rawdb_csv : _reader.py.__init__: line(85) : auto_io = True automatically creating or altering exception delta table landing.customer _context_id=eabae972-05cf-4e1d-8979-667ee888fa22
INFO : 2022-09-04 20:47:53,907 : customer_landing_to_rawdb_csv : _delta_lake.py.create_database: line(46) : Creating database if not exists `landing`
INFO : 2022-09-04 20:48:01,929 : customer_landing_to_rawdb_csv : _delta_lake.py.create_table: line(35) : Creating table if not exists landing.exceptions at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/exceptions/landing/exceptions
INFO : 2022-09-04 20:48:12,946 : customer_landing_to_rawdb_csv : _reader.py.read: line(301) : Reading data for landing.customer from file:/Users/shaunryan/AzureDevOps/yetl/data/landing/202207*/customer_202207*.csv with options {'mode': 'PERMISSIVE', 'inferSchema': False, 'header': True} _context_id=eabae972-05cf-4e1d-8979-667ee888fa22
INFO : 2022-09-04 20:48:13,155 : customer_landing_to_rawdb_csv : _reader.py.validate: line(284) : Validating dataframe read using PERMISSIVE corrupt column at _corrupt_record _context_id=eabae972-05cf-4e1d-8979-667ee888fa22
INFO : 2022-09-04 20:48:14,058 : customer_landing_to_rawdb_csv : _reader.py.validate: line(298) : {
    "validation": {
        "schema_on_read": {
            "landing.customer": {
                "total_count": 20,
                "valid_count": 20,
                "exception_count": 0
            }
        }
    }
}
INFO : 2022-09-04 20:48:14,059 : customer_landing_to_rawdb_csv : _factory.py.get_schema_repo_type: line(39) : Setting up schema repo on spark_schema_file 
INFO : 2022-09-04 20:48:14,060 : customer_landing_to_rawdb_csv : _spark_file_schema_repo.py.load_schema: line(58) : Loading schema for dataset landing.customer_preferences from /Users/shaunryan/AzureDevOps/yetl/config/schema/spark/landing/customer_preferences.yaml using <class 'yetl_flow.file_system._file_system.FileSystem'>
INFO : 2022-09-04 20:48:14,062 : customer_landing_to_rawdb_csv : _reader.py.__init__: line(85) : auto_io = True automatically creating or altering exception delta table landing.customer_preferences _context_id=eabae972-05cf-4e1d-8979-667ee888fa22
INFO : 2022-09-04 20:48:14,062 : customer_landing_to_rawdb_csv : _delta_lake.py.create_database: line(46) : Creating database if not exists `landing`
INFO : 2022-09-04 20:48:14,224 : customer_landing_to_rawdb_csv : _reader.py.create_or_alter_table: line(170) : Exception table already exists landing.exceptions at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/exceptions/landing/exceptions _context_id=eabae972-05cf-4e1d-8979-667ee888fa22
INFO : 2022-09-04 20:48:14,224 : customer_landing_to_rawdb_csv : _reader.py.read: line(301) : Reading data for landing.customer_preferences from file:/Users/shaunryan/AzureDevOps/yetl/data/landing/202207*/customer_preferences_202207*.csv with options {'mode': 'PERMISSIVE', 'inferSchema': False, 'header': True} _context_id=eabae972-05cf-4e1d-8979-667ee888fa22
INFO : 2022-09-04 20:48:14,248 : customer_landing_to_rawdb_csv : _reader.py.validate: line(284) : Validating dataframe read using PERMISSIVE corrupt column at _corrupt_record _context_id=eabae972-05cf-4e1d-8979-667ee888fa22
INFO : 2022-09-04 20:48:14,863 : customer_landing_to_rawdb_csv : _reader.py.validate: line(298) : {
    "validation": {
        "schema_on_read": {
            "landing.customer_preferences": {
                "total_count": 20,
                "valid_count": 20,
                "exception_count": 0
            }
        }
    }
}
INFO : 2022-09-04 20:48:14,864 : customer_landing_to_rawdb_csv : _factory.py.get_schema_repo_type: line(39) : Setting up schema repo on deltalake_sql_file 
INFO : 2022-09-04 20:48:14,864 : customer_landing_to_rawdb_csv : _deltalake_sql_file.py.load_schema: line(43) : Loading schema for dataset raw.customer from ./config/schema/deltalake/raw/customer.sql using <class 'yetl_flow.file_system._file_system.FileSystem'>
INFO : 2022-09-04 20:48:14,865 : customer_landing_to_rawdb_csv : _writer.py._get_conf_partitions: line(76) : Parsed partitioning columns from sql ddl for raw.customer as ['_partition_key', 'allow_contact']
INFO : 2022-09-04 20:48:14,865 : customer_landing_to_rawdb_csv : _writer.py.__init__: line(48) : auto_io = True automatically creating or altering delta table raw.customer
INFO : 2022-09-04 20:48:14,865 : customer_landing_to_rawdb_csv : _delta_lake.py.create_database: line(46) : Creating database if not exists `raw`
INFO : 2022-09-04 20:48:15,295 : customer_landing_to_rawdb_csv : _delta_lake.py.create_table: line(35) : Creating table if not exists raw.customer at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/raw/customer
You are setting a property: delta.autoOptimize.autoCompact that is not recognized by this version of Delta
You are setting a property: delta.autoOptimize.optimizeWrite that is not recognized by this version of Delta
INFO : 2022-09-04 20:48:30,913 : customer_landing_to_rawdb_csv : _context.py.__init__: line(78) : Checking spark and databricks versions
INFO : 2022-09-04 20:48:31,101 : customer_landing_to_rawdb_csv : _context.py.__init__: line(90) : Databricks Runtime version not detected.
INFO : 2022-09-04 20:48:31,101 : customer_landing_to_rawdb_csv : _context.py.__init__: line(92) : Spark version detected as : 3.3.0 f74867bddfbcdd4d08076db36851e88b15e66556
INFO : 2022-09-04 20:48:31,101 : customer_landing_to_rawdb_csv : _framework.py.wrap_function: line(36) : Executing Dataflow customer_landing_to_rawdb_csv with:
                timeslice=2022-07-* 00:00:00.000000 
                retries=0 
                save_type=DefaultSave
INFO : 2022-09-04 20:48:31,101 : customer_landing_to_rawdb_csv : main.py.customer_landing_to_rawdb_csv: line(35) : Joining customers with customer_preferences
INFO : 2022-09-04 20:48:31,129 : customer_landing_to_rawdb_csv : _writer.py.write: line(257) : Writing data to raw.customer at file:/Users/shaunryan/AzureDevOps/yetl/data/delta_lake/raw/customer
INFO : 2022-09-04 20:48:31,129 : customer_landing_to_rawdb_csv : _writer.py.write: line(264) : Auto compacting in memory partitions for raw.customer on partitions ['_partition_key', 'allow_contact']
INFO : 2022-09-04 20:48:31,624 : customer_landing_to_rawdb_csv : _destination.py.write: line(40) : IO operations for raw.customer will be paritioned by: 
{
    "_partition_key": [
        20220711,
        20220712
    ],
    "allow_contact": [
        false,
        true
    ]
}
INFO : 2022-09-04 20:48:31,624 : customer_landing_to_rawdb_csv : _save.py.write: line(28) : Writer saving using the DefaultSave which is the configured save = append.
INFO : 2022-09-04 20:48:35,581 : customer_landing_to_rawdb_csv : _writer.py.write: line(275) : Auto optimizing raw.customer where {'_partition_key': [20220711, 20220712], 'allow_contact': [False, True]} zorder by ['allow_contact', 'id']
INFO : 2022-09-04 20:48:35,581 : customer_landing_to_rawdb_csv : _delta_lake.py.optimize: line(128) : optimizing table raw.customer
OPTIMIZE `raw`.`customer` WHERE `_partition_key` in (20220711,20220712) and `allow_contact` in (False,True) ZORDER BY (`allow_contact`,`id`)
ERROR : 2022-09-04 20:48:36,017 : customer_landing_to_rawdb_csv : _framework.py.wrap_function: line(56) : Dataflow application customer_landing_to_rawdb_csv failed due to allow_contact is a partition column. Z-Ordering can only be performed on data columns.
