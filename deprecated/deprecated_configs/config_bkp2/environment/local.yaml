datalake: "{{cwd}}/data"
datalake_protocol: "file:"
spark:
  logging_level: ERROR
  config:
    spark.master: local
    # yetl uses table properties so this must be set as a table
    # property or globally like here in the spark context
    spark.databricks.delta.allowArbitraryProperties.enabled: true
    spark.jars.packages: io.delta:delta-core_2.12:2.1.0
    park.sql.extensions: io.delta.sql.DeltaSparkSessionExtension
    spark.sql.catalog.spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog
    spark.databricks.delta.merge.repartitionBeforeWrite.enabled: true

pipeline_repo:
  pipeline_file:
    pipeline_root: "./config/{{project}}/pipelines"
    sql_root: "./config/{{project}}/sql"

spark_schema_repo:
  spark_schema_file:
    spark_schema_root: ./config/schema/spark

deltalake_schema_repo:
  deltalake_sql_file:
    deltalake_schema_root: ./config/schema/deltalake

# used to write data lineage to
metadata_repo:
  metadata_file:
    metadata_root: ./config/runs
    metadata_dataset: dataset.json
    metadata_index: index.json
    